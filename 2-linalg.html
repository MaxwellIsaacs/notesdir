#+OPTIONS: tex:t
#+HTML_MATHJAX: t
#+title: Chapter 2 Notes
#+author: Max Isaacs

* Chapter 2
** Matrix Addition Review
- All matrices added or subtracted with each other need to be the same shape
- Matrix addition and subtraction hold the same properties as normal arthematic

** Matrix Multiplication Review
- Not communitive, doesn't share the same properties as normal arthematic

** Transposition Review
- Can be seen as a rotation about the main diagonal
  - Therefore, matrices that are symetric about their main diagonal are called symetric and are equal to their tranpose

** Vector addition Review
- We the linear combination of two vectors is "closed" since the sum will always be within R^n
- Can also be view geometrically (how else would you view vectors that matters?)
  - as a list is really about it

** Vector multiplication Review
- All linear systems can be viewed as a product between vector *a* and matrix *b*
  - This is by what is essentially factoring out the coefficient matrix, so you are left with the vector of variables (ex. x_1, x_2, x_3) times the matrix of coefficients
- You can consider a matrix a row of vectors, so applying a scalar product is as simple as the linear combination of these vectoorg-html-export-to-htmls (not exactly, linear combination is equal to the size of the vectors, but this would be just smushing them together)
- So a vector times a matrix is just each ordered element of the vector times each ordered vector of the matrix (since the matrix is just a row of columns)
  - For instance:
    [8 9 1]   [5]      [8]    []    []
    [2 0 1] * [8]  =  5[2] + 8[] + 9[]
    [6 7 1]   [9]      [6]    []    []
- Any given a_j column vector of matrix *A* multiplied by the column vector e_j of the identity matrix *I* is equal to the a_j





** Linear Tranformations
- Row of the matrix that's applied times the columns of the matrix that's transformed
- We represent a given linear transformation with the notation below. The power it's raised to just means the number of dimensions the matrix is in. All this means is that all other dimensions are in the whitespace.
        \[
        T : \mathbb{R}^n \to \mathbb{R}^m
        \]
        - /This raised two questions for me/
          1. Can n > m?
             I. Yes, but only with non square matrices, and we don't focus on those right now
          2. Are there just an infinite amount of dimensions in the null space?
             I. The null space isn't a place, it's what dimensions get shoved into the origin after the linear transformation. I think this means that the matrix consists of vectors that are linearly codependent (/it's linearly dependent retard/)? (*see X*) There are only infinite dimensions in the null space if you are dealing with infinite dimensions
             V. A better way to phrase the null space is the dimensions that get collpased into the origin, meaning they no longer contribute to the output. In a linear transformation the corresponding column would be 0.
             X. If some of the columns of the matrix are linearly dependent (meaning 2 columns add up to another) then one of the dimensions doesn't contribute anything new to the transformation, results in a larger null space (this is kind of a summary of what chatGPT said, I'm not sure what it fully means)
               i.  if you imagine the vectors added together /head to tail/, after two get added, the third just stays in place, because it is equal to the other two vectors added together. Therefore, the vector doesn't add a new dimension in space. When you make different combos of the vectors, they won't be in 3 dimensions (in a 3x3 matrix thats linearly dependent), meaning they're span 2d.
               v. As you should already know, null space + rank = n /number of columns/. For every linearly dependent column, one dimension becomes redundant and will 0 out in gaussian elimination. This decreases the rank by one, in turn increasing the null space by one.
- New vocab dropped:
  + *T_a* in the graphic below is refered to as the *matrix transformation induced* by A
        \[
        T_A : \mathbb{R}^n \to \mathbb{R}^m \quad \text{defined by} \quad T_A(x) = Ax \quad \text{for every} \quad x \in \mathbb{R}^n
        \]

- The rotation of \( \frac{\pi}{2}\) radians is equavalent to applying the matrix below:
        \begin{bmatrix}
        0 & -1 \\
        1 & 0
        \end{bmatrix}
  since you are moving the rotating the basis vectors 90 degrees to the left. If you notice, you are actually calculating the cos, -sin, sin, and cos of the angle. This is written below in a much less retarded format so it makes sense.
        \begin{bmatrix}
        \cos(\theta) & -\sin(\theta) \\
        \sin(\theta) & \cos(\theta)
        \end{bmatrix}
  What's crazy is this is only possible because of the unit circle (kind of). Think about rotating the basis vectors (which are just points on a circle/sphere with a radius of one) as the same as moving the unit circle triangle of radius one. Therefore, the cos and sin are the ratio at which the basis vectors change




  


** Vector addition and multiplication Review
** Vector addition and multiplication Review





*** Overarching Ideas of the Chapter
- A matrix is just a row of vectors. Any sort of matrix operation is just a transforming these three (or two, or four, etc) basis vectors
- Finding the homogenous solution to a linear equation (i.e setting a matrix times a vector equal to 0) is asking what vector falls in the null space of a given matrix.
- When multiplying a matrix by a vector /Ax = b/ we say A = r, where r is the rank of matrix A. If the rank of [A | b] = r+1, the system is inconsistent (an entire plane is in the null space while the vector /b/ doesn't equal z on that axis)

       /A/     /x/
    [8 9 11] [ 1]
    [2 0 1] [ 2]
    [0 0 0] [ 9]

        + /Why is this the case?/ The entire z plane gets smushed into the null space. Therefore, the vector /x/ is impossible after the given transformation, since it has a z value of 9. If the problem were asking for the homogenous solution, it wouldnt matter since the problem is asking for the null space anyways

- When a matrix
